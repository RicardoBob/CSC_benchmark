# -*- coding: utf-8 -*-
"""chess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HtcSlSKfma2AKAYileqp5ru7aYtU_fNV
"""

import numpy as np
import os
from numba import njit
from functools import reduce
from itertools import groupby
from math import ceil
import glob
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import random
from skimage.util.shape import view_as_blocks
from skimage import io, transform
from sklearn.model_selection import train_test_split, KFold
from keras.utils.vis_utils import plot_model
from keras import layers, models, optimizers
from keras.initializers import he_normal, lecun_normal
from keras import backend as K
import keras
import warnings
import tensorflow as tf

from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

from tqdm import tqdm_notebook as tqdm

print(tf.__version__)
print(tf.keras.__version__)

# for replicability purposes
tf.random.set_seed(91195003)
np.random.seed(91190530)

# for an easy reset backend session state
tf.keras.backend.clear_session()

#Paths used point to google drive
TRAIN_IMAGE_PATH="/content/drive/MyDrive/MMC/Chess/dataset/train"
TEST_IMAGE_PATH="/content/drive/MyDrive/MMC/Chess/dataset/test"

SQUARE_SIZE = 25
train_size = 2400
test_size = 400
BATCH_SIZE= 64
SEED=3123123
Epoch=10
k_folds=3
PATIENCE=4

class EarlyStoppingAtMinLoss(keras.callbacks.Callback):
    """Stop training when the loss is at its min, i.e. the loss stops decreasing.

  Arguments:
      patience: Number of epochs to wait after min has been hit. After this
      number of no improvement, training stops.
  """

    def __init__(self, patience=PATIENCE):
        super(EarlyStoppingAtMinLoss, self).__init__()
        self.patience = patience
        # best_weights to store the weights at which the minimum loss occurs.
        self.best_weights = None

    def on_train_begin(self, logs=None):
        # The number of epoch it has waited when loss is no longer minimum.
        self.wait = 0
        # The epoch the training stops at.
        self.stopped_epoch = 0
        # Initialize the best as infinity.
        self.best = np.Inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get("loss")
        if np.less(current, self.best):
            self.best = current
            self.wait = 0
            # Record the best weights if current results is better (less).
            self.best_weights = self.model.get_weights()
        else:
            self.wait += 1
            if self.wait >= self.patience:
                self.stopped_epoch = epoch
                self.model.stop_training = True
                print("Restoring model weights from the end of the best epoch.")
                self.model.set_weights(self.best_weights)

    def on_train_end(self, logs=None):
        if self.stopped_epoch > 0:
            print("Epoch %05d: early stopping" % (self.stopped_epoch + 1))

def get_image_filenames(image_path, image_type):
    if(os.path.exists(image_path)):
        return glob.glob(os.path.join(image_path, '*.'+image_type))
    return

train = get_image_filenames(TRAIN_IMAGE_PATH, "jpeg")#train
test = get_image_filenames(TEST_IMAGE_PATH, "jpeg")#test

random.shuffle(train)
random.shuffle(test)

train = train[:train_size]
test = test[:test_size]

def fen_from_filename(filename):
  base = os.path.basename(filename)
  return os.path.splitext(base)[0]

f, axarr = plt.subplots(1,3, figsize=(120, 120))

for i in range(0,3):
    axarr[i].set_title(fen_from_filename(train[i]), fontsize=70, pad=30)
    axarr[i].imshow(mpimg.imread(train[i]))
    axarr[i].axis('off')

def one_hot_encode(fen):
    pieces = "PNBRQKpnbrqk"
    pieces_as_vectors = np.eye(13)
    encoded_label = np.empty((0,13))
    board_rows = fen.split('-')
    for row in board_rows:
      for elem in row:
        if elem.isdigit():
          empties = np.tile(pieces_as_vectors[12],(int(elem),1))
          encoded_label=np.append(encoded_label,empties,axis=0)
        else:
          encoded_label=np.append(encoded_label,pieces_as_vectors[pieces.index(elem)].reshape((1,13)),axis=0)
    return encoded_label

@njit
def index(array, item):
    for idx, val in np.ndenumerate(array):
        if val == item:
            return idx

pieces = "PNBRQKpnbrqk"

def indeces(row):
  return np.array(list(map(lambda vec: 1 if vec[12]==1 else pieces[index(vec,1)[0]],row)))

def aux(k,group):
  if str(k).isdigit():
    return str(sum(map(lambda x: x.astype(int),group)))
  else:
    return "".join(group)
  
def one_hot_decode(encoded_label):
  label=""
  board = np.reshape(encoded_label,(8,8,13))
  for row in board:
    empties_and_pieces = indeces(row)
    label+= "".join([aux(k,group) for k,group in groupby(empties_and_pieces)])
    label+='-'
  return label[:-1]

def isomorphism_check():
  ''' Function used to check if one_hot_decode(one_hot_encode()) = ID '''
  for x in train:
    label = fen_from_filename(x)
    if not(label == one_hot_decode(one_hot_encode(label))):
      return False
  return True

one_hot_decode(one_hot_encode("2B1n3-1r6-2K5-7b-1p6-p6k-3N4-8"))

isomorphism_check()

def process_image(img):
    downsample_size = SQUARE_SIZE*8
    square_size = SQUARE_SIZE
    img_read = io.imread(img)
    img_read = transform.resize(img_read, (downsample_size, downsample_size), mode='constant')
    tiles = view_as_blocks(img_read, block_shape=(square_size, square_size, 3))
    tiles = tiles.squeeze(axis=2)
    return tiles.reshape(64, square_size, square_size, 3)

def plot_learning_curves(history, epochs):
    #accuracies and losses
    acc = [0] * epochs
    val_acc = [0] * epochs
    loss = [0] * epochs
    val_loss = [0] * epochs
    for h in history:
        acc = np.add(acc, h.history['accuracy'])
        val_acc = np.add(val_acc, h.history['val_accuracy'])
        loss = np.add(loss, h.history['loss'])
        val_loss = np.add(val_loss, h.history['val_loss'])
    for i in range(epochs):
        acc[i] /= len(history)
        val_acc[i] /= len(history)
        loss[i] /= len(history)
        val_loss[i] /= len(history)
    epochs_range = range(epochs)
    # creating figure
    plt.figure(figsize=(8, 8))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Training/Validation Accuracy')
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Training/Validation Loss')

def get_model(image_size):
    input_shape = (64,image_size,image_size,3)
    model = models.Sequential()
    model.add(layers.Conv2D(16, 5, activation='relu', kernel_initializer='he_normal', input_shape=input_shape[1:]))
    model.add(layers.Dropout(0.2))
    model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'))
    model.add(layers.Dropout(0.2))
    model.add(layers.MaxPooling2D(pool_size=(2, 2), padding='same'))
    model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'))
    model.add(layers.Dropout(0.2))
    model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal'))
    model.add(layers.Dropout(0.2))
    model.add(layers.Flatten())
    model.add(layers.Dense(128, activation='relu', kernel_initializer='he_normal'))
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(13, activation='softmax', kernel_initializer='lecun_normal'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    
    return model

def train_gen(features, batch_size):
    i=0
    while True:
        batch_x=[]
        batch_y=[]
        for b in range(batch_size):
            if i==len(features):
                i=0
                random.shuffle(features)
            img=str(features[i])
            y = one_hot_encode(fen_from_filename(img))
            x = process_image(img)
            for x_part in x:
                batch_x.append(x_part)
            for y_part in y:
                batch_y.append(y_part)
            i+=1
        yield (np.array(batch_x), np.array(batch_y))

def pred_gen(features, batch_size):
    for i, img in enumerate(features):
        yield process_image(img)

def compile_and_fit(model,X,batch_size,epochs,n_folds):
  kf = KFold(n_splits=n_folds)
  j=1
  histories = []
  for (train_fold, valid_fold) in kf.split(X):
    print("K-FOLD: ",str(j))
    history=model.fit(train_gen([train[i] for i in (train_fold)], batch_size=batch_size),\
                      steps_per_epoch=ceil(train_size*(1-1/n_folds)/batch_size), epochs=epochs,
                      validation_data=train_gen([train[i] for i in (valid_fold)], batch_size=batch_size),
                      validation_steps=ceil(train_size/n_folds/batch_size), verbose=1, shuffle=False,
                      callbacks=[EarlyStoppingAtMinLoss()])
    histories.append(history)
    j+=1
  return model,histories

cnn1 = get_model(SQUARE_SIZE)
cnn1,history = compile_and_fit(cnn1,train,BATCH_SIZE,Epoch,k_folds)

plot_learning_curves(history,Epoch)

final_res = (
  cnn1.predict(pred_gen(test, 64), steps=test_size)
  .argmax(axis=1)
)

eye = np.eye(13)
number_to_vec = lambda elem: eye[elem]
out = number_to_vec(final_res).reshape((test_size,8,8,13))

pred_fens = np.array([one_hot_decode(one_hot) for one_hot in out])
test_fens = np.array([fen_from_filename(fn) for fn in test])
final_accuracy = (pred_fens == test_fens).astype(float).mean()
print("Final Accuracy: {:1.5f}%".format(final_accuracy))

"""**MLP**"""

def train_gen_mlp(features, batch_size):
    i=0
    while True:
        batch_x=[]
        batch_y=[]
        for b in range(batch_size):
            if i==len(features):
                i=0
                random.shuffle(features)
            img=str(features[i])
            y = one_hot_encode(fen_from_filename(img))
            x = process_image(img)
            x = x.reshape((64,np.prod(x.shape[1:])))
            for x_part in x:
                batch_x.append(x_part)
            for y_part in y:
                batch_y.append(y_part)
            i+=1
        yield (np.array(batch_x), np.array(batch_y))

def pred_gen_mlp(features, batch_size):
    for i, img in enumerate(features):
        res = process_image(img)
        yield res.reshape((64,np.prod(res.shape[1:])))

def get_model_mlp(image_size):#(model_name, image_size)
    model = models.Sequential()
    model.add(layers.Dense(128, activation='relu', input_shape=(image_size*image_size*3,)))
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.Dense(13, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    return model

def fit_mlp(model,X,batch_size,epochs,n_folds):
  kf = KFold(n_splits=n_folds)
  j=1
  histories = []
  for (train_fold, valid_fold) in kf.split(X):
    print("K-FOLD: ",str(j))
    history=model.fit(train_gen_mlp([train[i] for i in train_fold], batch_size=batch_size),
                      steps_per_epoch=ceil(train_size*(1-1/n_folds)/batch_size), epochs=epochs,
                      validation_data=train_gen_mlp([train[i] for i in valid_fold], batch_size=batch_size),
                      validation_steps=ceil(train_size/n_folds/batch_size), verbose=1, shuffle=False,
                      callbacks=[EarlyStoppingAtMinLoss()])
    histories.append(history)
    j+=1
  return model,histories

mlp = get_model_mlp(SQUARE_SIZE)
mlp,history = fit_mlp(mlp,train,BATCH_SIZE,Epoch,k_folds)

plot_learning_curves(history,Epoch)

final_res = (
  mlp.predict(pred_gen_mlp(test, BATCH_SIZE), steps=test_size)
  .argmax(axis=1)
)

out = number_to_vec(final_res).reshape((test_size,8,8,13))
pred_fens = np.array([one_hot_decode(one_hot) for one_hot in out])
test_fens = np.array([fen_from_filename(fn) for fn in test])
final_accuracy = (pred_fens == test_fens).astype(float).mean()
print("Final Accuracy: {:1.5f}%".format(final_accuracy))

def get_model_mlp2(image_size):#(model_name, image_size)
    model = models.Sequential()
    model.add(layers.Dense(32, activation='relu', input_shape=(image_size*image_size*3,)))
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(13, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    return model

mlp2 = get_model_mlp2(SQUARE_SIZE)
mlp2,history = fit_mlp(mlp2,train,BATCH_SIZE,Epoch,k_folds)

final_res = (
  mlp2.predict(pred_gen_mlp(test, BATCH_SIZE), steps=test_size)
  .argmax(axis=1)
)

out = number_to_vec(final_res).reshape((test_size,8,8,13))
pred_fens = np.array([one_hot_decode(one_hot) for one_hot in out])
test_fens = np.array([fen_from_filename(fn) for fn in test])
final_accuracy = (pred_fens == test_fens).astype(float).mean()
print("Final Accuracy: {:1.5f}%".format(final_accuracy))

plot_learning_curves(history,Epoch)

def get_last_cnn(image_size):
    input_shape = (64,image_size,image_size,3)
    model = models.Sequential()
    model.add(layers.Conv2D(16, 8, activation='relu', kernel_initializer='he_normal', input_shape=input_shape[1:]))
    model.add(layers.Dropout(0.2))
    model.add(layers.Conv2D(16, 4, activation='relu', kernel_initializer='he_normal'))
    model.add(layers.Dropout(0.2))
    model.add(layers.MaxPooling2D(pool_size=(2, 2), padding='same'))
    model.add(layers.Conv2D(16, 2, activation='relu', kernel_initializer='he_normal'))
    model.add(layers.Dropout(0.2))
    model.add(layers.Flatten())
    model.add(layers.Dense(256, activation='relu', kernel_initializer='he_normal'))
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(13, activation='softmax', kernel_initializer='lecun_normal'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    
    return model

cnn2 = get_last_cnn(SQUARE_SIZE)
cnn2,history = compile_and_fit(cnn2,train,BATCH_SIZE,Epoch,k_folds)

plot_learning_curves(history,Epoch)

final_res = (
  cnn1.predict(pred_gen(test, 64), steps=test_size)
  .argmax(axis=1)
)

out = number_to_vec(final_res).reshape((test_size,8,8,13))
pred_fens = np.array([one_hot_decode(one_hot) for one_hot in out])
test_fens = np.array([fen_from_filename(fn) for fn in test])
final_accuracy = (pred_fens == test_fens).astype(float).mean()
print("Final Accuracy: {:1.5f}%".format(final_accuracy))