# -*- coding: utf-8 -*-
"""ucf101.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WYNJ6RaLfmcK3hW3v6KiGBjXq5EO4vyP
"""

import os
import cv2
import glob
import tensorflow as tf
import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow
import pandas as pd
from keras import layers, models, optimizers
from skimage.transform import resize
from keras import backend as K
from math import ceil
import numpy as np
from sklearn.model_selection import train_test_split, KFold
import random

print(tf.__version__)
print(tf.keras.__version__)

# for replicability purposes
tf.random.set_seed(91195003)
np.random.seed(91190530)

# for an easy reset backend session state
tf.keras.backend.clear_session()

#Paths used point to google drive
TRAIN_IMAGE_TXT_PATH="/content/drive/MyDrive/MMC/UCF101/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/trainlist01.txt"
TEST_IMAGE_TXT_PATH="/content/drive/MyDrive/MMC/UCF101/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/testlist01.txt"
CATEGS_IMAGE_TXT_PATH="/content/drive/MyDrive/MMC/UCF101/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/classInd.txt"
VIDEOS_PATH = '/content/drive/MyDrive/MMC/UCF101/UCF-101/'

#Vari√°veis globais
SIZE = (128, 128)
CHANNELS = 3
NBFRAME = 6
BATCH_SIZE = 16
NUM_CLASSES = 3
EPOCHS=4
N_FOLDS=3

train_df = pd.read_csv(TRAIN_IMAGE_TXT_PATH,sep = " ",header = None,names = ['path','class'])
test_df = pd.read_csv(TEST_IMAGE_TXT_PATH,sep = " ",header = None,names = ['path'])
category = pd.read_csv(CATEGS_IMAGE_TXT_PATH,sep = " ",header = None,names = ['category','class'])

#Convert a class name to a number
def class_to_category(str):
  for i in range(len(category)):
    if category.iloc[i]['class']==str.split('/')[0]:
      return category.iloc[i]['category']-1
  return -1

#build dataset limiting the number of examples per class
X_train = []
i = 0
n = 50
while len(X_train) < NUM_CLASSES and i < 102:
    if len(train_df[train_df['class']==i]) >= n:
        df_temp = train_df[train_df['class']==i].iloc[:n]
        path = df_temp['path']
        X_train.append(path)        
    i += 1

X_train = np.array(X_train)

X_train.shape

X_train= X_train.reshape(NUM_CLASSES*n,)
y_train = np.array([class_to_category(s) for s in X_train])

test_df['class']=[class_to_category(s) for s in test_df['path']]
test_df

#build dataset limiting the number of examples per class
X_test = []
i = 0
n = 10
while len(X_test) < NUM_CLASSES and i < 102:
    if len(test_df[test_df['class']==i]) >= n:
        df_temp = test_df[test_df['class']==i].iloc[:n]
        path = df_temp['path']
        X_test.append(path)        
    i += 1

X_test = np.array(X_test)

X_test= X_test.reshape(NUM_CLASSES*n,)
y_test = np.array([class_to_category(s) for s in X_test])

#check if the classes are the same in train and test split
print(set(y_test))
set(y_test)==set(y_train)

def read_frames(root_folder,path,each_nth=10):   
    vcap=cv2.VideoCapture(root_folder+path)
    success=True

    frames=[]
    cnt=0
    while success:
        try:
          success,image=vcap.read()
          if not(success):
            break
          cnt+=1
          if cnt%each_nth==0:
            image=resize(image,(128,128))
            frames.append(image)
        except Exception as e:
            print(e,root_folder+path)
    
    return frames

def select_frames(frames_arr , n=NBFRAME):
    frames=[]
    for t in np.linspace(0, len(frames_arr)-1, num=n):
        frames.append(frames_arr[int(t)])
    
    frames = np.array(frames)
    return frames

fig = plt.figure(figsize=(32,8))
example_frames = select_frames(read_frames(VIDEOS_PATH,"BabyCrawling/v_BabyCrawling_g09_c06.avi"))
for i,image in enumerate(example_frames):
  ax = plt.subplot(2,5,i+1)
  imshow(image)

def train_gen(features,batch_size):
  i=0
  while True:
      batch_x=[]
      batch_y=[]
      for b in range(batch_size):
          if i==len(features):
              i=0
              random.shuffle(features)
          video_path=features[i]
          y = class_to_category(video_path)
          x = select_frames(read_frames(VIDEOS_PATH,video_path))
          batch_x.append(x)
          batch_y.append(y)
          i+=1
      yield (np.array(batch_x), np.array(batch_y))

def pred_gen(features):
    for video_path in features:
        yield select_frames(read_frames(VIDEOS_PATH,video_path)).reshape(1,NBFRAME,128,128,3)

def cnn_model():
  model = models.Sequential()
  model.add(layers.Conv3D(8,16,padding='same',activation='relu',kernel_initializer='he_normal',input_shape=(NBFRAME,128,128,3)))
  model.add(layers.Conv3D(8,8,padding='same',activation='relu',kernel_initializer='he_normal'))
  model.add(layers.Flatten())
  model.add(layers.Dense(units=NUM_CLASSES,activation='softmax'))
  model.summary()
  model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['accuracy'])
    
  return model

def compile_and_fit(model,X,batch_size,epochs,n_folds):
  kf = KFold(n_splits=n_folds)
  j=1
  histories = []
  for (train_fold, valid_fold) in kf.split(X):
    print("=========================================")
    print("====== K Fold Validation step => %d/%d =======" % (j,n_folds))
    print("=========================================")
    x_train = X[train_fold]
    x_val = X[valid_fold]
    history=model.fit(train_gen(x_train,batch_size), steps_per_epoch=(x_train.shape[0]//batch_size),epochs=epochs,\
                      validation_data=train_gen(x_val,batch_size),validation_steps=(x_val.shape[0]//batch_size), verbose=1, shuffle=False)
    histories.append(history)
    j+=1
  return model,histories

conv_net_3d = cnn_model()
conv_net_3d,history = compile_and_fit(conv_net_3d,X_train,BATCH_SIZE,EPOCHS,N_FOLDS)

final_res = (
  conv_net_3d.predict(pred_gen(X_test),batch_size=1, steps=len(X_test))
  .argmax(axis=1)
)

final_accuracy = sum(final_res == y_test)/len(y_test)
print("Final Accuracy: {:1.5f}%".format(final_accuracy))

def build_convnet(shape=(128, 128, 3)):
    momentum = .9
    model = models.Sequential()
    model.add(layers.Conv2D(64, (3,3), input_shape=shape, padding='same', activation='relu'))
    model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization(momentum=momentum))
    model.add(layers.MaxPool2D())    
    model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
    model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization(momentum=momentum))
    model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
    model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization(momentum=momentum))
  
    model.add(layers.GlobalMaxPool2D())
    return model

def CNN_GRU(shape=(NBFRAME, 128, 128, 3)):
    convnet = build_convnet(shape[1:])
    model = models.Sequential()
    model.add(layers.TimeDistributed(convnet, input_shape=shape))
    model.add(layers.GRU(64))
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.Dropout(.2))
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.Dropout(.2))
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(NUM_CLASSES, activation='softmax'))
    model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=['accuracy'])
    model.summary()
    return model

cnn_gru = CNN_GRU()
cnn_gru,history = compile_and_fit(cnn_gru,X_train,BATCH_SIZE,EPOCHS,N_FOLDS)

final_res = (
  cnn_gru.predict(pred_gen(X_test),batch_size=1, steps=len(X_test))
  .argmax(axis=1)
)
final_accuracy = sum(final_res == y_test)/len(y_test)
print("Final Accuracy: {:1.5f}%".format(final_accuracy))

